# @package _global_
defaults:
  - override /model/config: causal-small
  - _self_
model_arch: transformer
model:
  config: # redundant but included for completeness
    ffn_dim: 768
    hidden_size: 768
    num_attention_heads: 8
    num_hidden_layers: 8
    max_position_embeddings: 512
