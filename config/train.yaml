defaults:
  - /train-from-config
  - _self_
corpus_name: ???
model:
  config:
    vocab_size: 50002
model_arch: ???
root_dir: ${hydra:runtime.cwd} # can override to change paths below without affecting hydra.runtime.cwd
run_name_suffix: "" # just in case- can be overriden to customize run name
tokenizer:
  tokenizer_file: ${root_dir}/models/tokenizer/gulordava.json
  model_max_length: 512
training_args:
  hub_model_id: CLMBR/${hydra:job.name}
  hub_strategy: all_checkpoints
  push_to_hub: true
  hub_always_push: true
  num_train_epochs: 40
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  run_name: ${hydra:job.name}
  load_best_model_at_end: true
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  logging_strategy: "steps"
  logging_first_step: true
  logging_steps: 512
dataset:
  name: ${corpus_name}
dynamic_resume: true
use_iterable_dataset: true
# train_shards_per_worker: 2
# eval_shards_per_worker: 2
hydra:
  run:
    dir: ./outputs/preemption_testing/${run_name_suffix}/${corpus_name}/${model_arch}/${seed}
    # dir: ${hydra:job.name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  searchpath:
    - file://${oc.env:PWD}/lm-training/config
  job:
    env_set:
      TOKENIZERS_PARALLELISM: false
      WANDB_USERNAME: abhinav-patil
      WANDB_PROJECT: preemption-testing
      WANDB_NAME: ${hydra:job.name}
      WANDB_JOB_TYPE: ${split:${hydra:job.name},-,2} # model_arch
      WANDB_RUN_GROUP: ${split:${hydra:job.name},-,1} # corpus_name
      WANDB_TAGS: ${split:${hydra:job.name},-,1},${split:${hydra:job.name},-,2},${split:${hydra:job.name},-,3}
    name: preemption_testing-${corpus_name}-${model_arch}-${seed}${run_name_suffix}
